# Домашнее задание к занятию "3.6. Компьютерные сети, лекция 2"

>1. На лекции мы обсудили, что манипулировать размером окна необходимо для эффективного наполнения приемного буфера участников TCP сессии (Flow Control). Подобная проблема в полной мере возникает в сетях с высоким RTT. Например, если вы захотите передать 500 Гб бэкап из региона Юга-Восточной Азии на Восточное побережье США. [Здесь](https://www.cloudping.co/grid) вы можете увидеть и 200 и 400 мс вполне реального RTT. Подсчитайте, какого размера нужно окно TCP чтобы наполнить 1 Гбит/с канал при 300 мс RTT (берем простую ситуацию без потери пакетов). Сколько потребуется времени при этом для передачи 500 Гб?


Полоса пропускания (бит/сек) * RTT (круговое время передачи по сети) = размер окна в битах  
 
 X байт x 8=1 Гбит/с x 0.3с  
 X=0.0375 x 10^9=37.5 Мбайт (Размер окна)  
Время передачи 500Гб (упрощенно)= 500Гб/1Гбит=500x 1024^3/1000^3*8сек= 500 x 1.073741824 x 8сек=4295сек=1 час 11.5 мин.  



>2. В чем состоит опасность слишком большого размера окна? 

«Засорение сети множеством пакетов», образование очереди, следствие потеря пакетов в связи с переполнением буфера принимающего устройства или превышения допустимого ожидания подтверждения. Все это приводит к потере пакетов.  
Но данная проблема устраняется алгоритмами изменения окна в зависимости от потерь и RTT.   


>Во сколько раз упадет пропускная способность канала, если будет 1% потерь пакетов при передаче?


Тут подумал о сокращении TCP-окна в связи с потерями, и затратами по повторную отправку 1% = по RTT.  

Нашел уравнение Mathis. (но как понял не до конца сюда подходит):  
«Существует три основных показателя, которые влияют на пропускную способность соединения TCP: время прохождения сигнала в обоих направлениях ( RTT ), потеря пакетов ( p ) и максимальный размер сегмента ( MSS ). Хорошей моделью, которая описывает взаимосвязь этих трех переменных пропускной способности, является уравнение Mathis.»  
Из T <= (MSS/RTT)×(1/SQRT{p})   взято на основании The Macroscopic Behavior of the TCP Congestion Avoidance Algorithm.  

Цитата из этой статьи:   
при вероятности потери пакета 1% и RTT=50 мсек максимальная пропускная способность будет меньше 3 Мбит/с вне зависимости от широкополосности канала.  






>3. Какая  максимальная реальная скорость передачи данных достижима при линке 100 Мбит/с? Вопрос про TCP payload, то есть цифры, которые вы реально увидите в операционной системе в тестах или в браузере при скачивании файлов. Повлияет ли размер фрейма на это?


Так как при прохождении данных учитывается размер фреймов,  то для вычисления прохождения полезных данных нужно отнять заголовки,  и получим MMS.  
Стандартный пример при MTU=1500  
Фрейм Ethernet = 1538 = 1500 (MTU) + 14 (заголовок Ethernet) + 4 (CRC32) + 12 (Inter-Frame Gap) + 7 (преамбула) + 1 ( Start of Frame Delimiter)).  
MMS= MTU(1500 – 20 (IP-заголовок) – 20 (TCP-заголовок)) = 1460.  
Сколько фреймов проходит через 100 Мбит/сек:   
FPS = (100/ (1538*8)) = 8127 бит/сек   

TCP payload (пропускная способность) = 1460(MMS) *8 * 8127fps = 94,9 Мбит/сек  
Размер фрейма повлияет, так как относительный размер заголовков к полезным данным MMS уменьшится.     




>4. Что на самом деле происходит, когда вы открываете сайт? :)
На прошлой лекции был приведен сокращенный вариант ответа на этот вопрос. Теперь вы знаете намного больше, в частности про IP адресацию, DNS и т.д.
Опишите максимально подробно насколько вы это можете сделать, что происходит, когда вы делаете запрос `curl -I http://netology.ru` с вашей рабочей станции. Предположим, что arp кеш очищен, в локальном DNS нет закешированных записей.


1)Попытка определения IP-адреса целевого хоста.   
Проверяем  /etc/nsswiych.conf на приоритет поиска, обычно первым проверяется /etc/hosts потом /etc/resolv.conf из которого по приоритету обращаемся обращаемся  локальному кешу dns,  если там нет необходимой записи,  то настройки ищем настройки DNS на интерфейсе, например в NetworkManager, берем первый указанный DNS-сервер.    
2) DNS-сервер в той же сети.    
Делаем запрос к DNS-серверу, если DNS-сервера в той же сети (определяется настройке интерфейсов, IP-адрес, маска сети), то смотрим локальную таблицу ARP (там нет записи) - делаем ARP запрос в подсеть, по связанному с ней интерфейсу, получаем ответ (попутно заносим запись MAC  в таблицу ARP), как правило это будет через Свитч который перенаправит запрос используя свою таблицу соотношения портом и MAC (например, возможна цепочка из нескольких устройств «Свитч»).   
3) DNS-сервер в другой подсети.    
Тогда отправляем ARP запрос на шлюз (в зависимости от таблицы маршрутизации на нашей машине, скорее всего на default gateway). Далее на основе таблиц маршрутизации стоящих устройств за шлюзом запрос дойдет до DNS-сервера.    
Если в кеше DNS-сервера есть нужная запись A-запись (AAAA-запись, возможно), то сервер её вернет.    
Если нет, зависит от настроек DNS-сервера, или он возьмет работу на себя (рекурсивный запрос или вернет адрес другого вышестоящего DNS-сервера) и будет обращаться по цепочке DNS-серверов, пока не найдет запись в их кеше или в крайнем случае обратится к серверу «точка» тот скажет где  .ru (обратится к нему), ru скажет кому делегирован домен netology.ru (серея итеративных запросов от .), от него получит ответ и вернет результат - записи А на нашу машину.    
Если с первым адресом DNS на интерфейсе, что-то не так, то берем второй DNS-сервер из списка и тоже самое (пока не получим результат или ошибку).   
3) Когда мы получили IP-адрес http://netology.ru, то по аналогии с поиском DNS-сервера отправляем пакет на http://netology.ru для начала установления соединения (SYNC , обратно SYNC-ACT, ACT). Далее идут пакеты с Seq и подтверждающий Act. Клиент отправляет Get-запрос на получения содержимого веб страницы.     
GET / HTTP/1.1   
Host: http://netology.ru   
..Другие заголовки    

Ответ от сервера, на согласие на протокол:   
HTTP/1.1200 OK   
.. Заголовки   
..Запрошеная страница  

И завершение соединения со стороны клиента FIN , обратно FIN-ACT, с клиента  ACT  (если сервер не дождался ACT, то закрытие по timeout).    




>5. Сколько и каких итеративных запросов будет сделано при резолве домена `www.google.co.uk`?


Четыре:   
1).  
2)uk.  
3)co.uk.  
4)google.co.uk.  



>6. Сколько доступно для назначения хостам адресов в подсети `/25`? А в подсети с маской `255.248.0.0`. Постарайтесь потренироваться в ручных вычислениях чтобы немного набить руку, не пользоваться калькулятором сразу.


126 (в том числе один скорее всего будет отдан шлюзу)    



>7. В какой подсети больше адресов, в `/23` или `/24`?


В /23  


>8. Получится ли разделить диапазон `10.0.0.0/8` на 128 подсетей по 131070 адресов в каждой? Какая маска будет у таких подсетей?


Маска =  /15    или 255.254.0.0   
32-15=17  свободный байт   
2^17 = 131072 – 2 (адрес сети, широковещательный)   

**10.0.0.0/8   =   00001010.0000000**0.00000000.00000000 /8   
15-8=7 байт высвобождаем    
2^7 = 128 сетей   


>9. ipvs. Если при запросе на VIP сделать подряд несколько запросов (например, `for i in {1..50}; do curl -I -s 172.28.128.200>/dev/null; done `), ответы будут получены почти мгновенно. Тем не менее, в выводе `ipvsadm -Ln` еще некоторое время будут висеть активные `InActConn`. Почему так происходит? Какое точно время и как на него повлиять? В каких случаях конфигурация по умолчаню на балансере может оказаться проблемой?
>Почему так происходит?

Влияние ожидания сервера - TIME_WAIT перед окончательным - закрытием соединения.  

>Какое точно время и как на него повлиять?

2 минуты  

Как понимаю, теми же параметрами ядра, что обсуждались в прошлом задании: 
**net.ipv4.tcp_tw_reuse и net.ipv4.tcp_tw_recycle**  
эти два параметра, позволяющие нарушать требования TCP-протокола, высвобождая соединения из TIME_WAIT раньше положенного срока. Обе эти опции базируются на расширении TCP-timestamps (маркировки пакетов относительными временными метками).  
«net.ipv4.tcp_tw_reuse позволяет использовать находящееся в TIME_WAIT соединение для нового исходящего подключения. При этом у нового подключения TCP timestamp должен быть на порядок больше чем последнее значение в предыдущей сессии. В этом случае сервер сможет отличить «опоздавший» пакет из предыдущего подключения от актуального. Использование параметра в большинстве случаев безопасно. Проблемы могут возникнуть в случае наличие «отслеживающего» firewall по пути, который решить не пропустить пакет в соединении, которое должно находиться в TIME_WAIT.»  
«net.ipv4.tcp_tw_recycle сокращает время нахождения подключения в очереди TIME_WAIT до значения RTO (Re-Transmission Time-Out), которое вычисляется на основании Round-Trip-Time (RTT) и разброса этой величины. При этом в ядре сохраняется последнее значение TCP-timestamp и пакеты с меньшим значением просто отбрасываются. Эта опция сделает сервис недоступным для клиентов за NAT в случае если при трансляции «пропускаются» TCP-timestamp от клиентов (в случае если NAT их удаляет, или заменяет на свои — проблем не будет). Поскольку предвидеть настройки внешних устройств не представляется возможным, данная опция настоятельно не рекомендуется к включению на серверах, доступных из Internet. При этом на «внутренних» серверах, где NAT нет (или же используется вариант 1-в-1) опция безопасна.»  


>В каких случаях конфигурация по умолчаню на балансере может оказаться проблемой?
   
При большом количестве подключений, когда количество исходящих подключений от балансировщика достигнет ограничения (менее 30тыс открытых исходящих портов)   




>10. Воспользуйтесь приложенным Vagrantfile, чтобы воспроизвести описанную конфигурацию ipvs у себя в рабочем окружении. Один из недостатков описанной конфигурации – отсутствие проверок живости (healthcheck) для бэкенд-серверов ipvs. Добавьте подобные проверки.

Добавил адрес балансировщика в keepalive, аналогично как показывали на лекции (на втором меняется MASTER => BACKUP, priority 100 => priority 50), проверил, работает:  
vrrp_script chk_nginx {  
script "/usr/bin/systemctl status nginx"  
interval 2  
}  
vrrp_instance VI_1 {  
state MASTER  
interface eth1  
virtual_router_id 1  
priority 100  
advert_int 1  
authentication {  
auth_type PASS  
auth_pass netology_secret  
}  
virtual_ipaddress {  
172.28.128.200/24 dev eth1  
}  
track_script {  
chk_nginx    
}  
}  




>11. **Дополнительное задание вне зачета.** На лекции мы познакомились отдельно с ipvs и отдельно с keepalived. Воспользовавшись этими знаниями, совместите технологии вместе (VIP должен подниматься демоном keepalived). Приложите конфигурационные файлы, которые у вас получились, и продемонстрируйте работу получившейся конструкции.

**0)Первоначальные настройки**  

**/etc/ipvsadm.rules**    
_-A -t 172.28.128.200:http -s rr  
-a -t 172.28.128.200:http -r 172.28.128.10:http -g -w 1  
-a -t 172.28.128.200:http -r 172.28.128.60:http -g -w 1_   
 
**netlogy1
/etc/keepalived/keepalived.conf**
https://github.com/syatihoko/devops-netology/blob/master/HomeWorks/11_03-keepalived-1.txt

**netlogy2
/etc/keepalived/keepalived.conf**
https://github.com/syatihoko/devops-netology/blob/master/HomeWorks/11_03-keepalived-2.txt

**1)	Штатная работа  
netology1# ip -4 a | grep 172**  
    inet 172.28.128.200/32 scope global lo  
    inet 172.28.128.11/24 brd 172.28.128.255 scope global eth1  
    inet 172.28.128.200/24 scope global secondary eth1  
    inet 172.28.128.10/24 scope global secondary eth1:1  

**netology2# ip -4 a | grep 172**  
    inet 172.28.128.200/32 scope global lo  
    inet 172.28.128.61/24 brd 172.28.128.255 scope global eth1  
    inet 172.28.128.60/24 scope global secondary eth1:2  

**netology3 # for i in {1..50}; do curl -I -s 172.28.128.200>/dev/null; done**  

**netology1 # wc -l /var/log/nginx/access.log**  
25 /var/log/nginx/access.log  

**netology2 # wc -l /var/log/nginx/access.log**  
25 /var/log/nginx/access.log  

**2)Отключаем netology1**  

**netology1# systemctl stop nginx  
netology1# ip -4 a | grep 172**  
    inet 172.28.128.200/32 scope global lo  
    inet 172.28.128.11/24 brd 172.28.128.255 scope global eth1  


**netology2# ip -4 a | grep 172**  
    inet 172.28.128.200/32 scope global lo  
    inet 172.28.128.61/24 brd 172.28.128.255 scope global eth1  
    inet 172.28.128.60/24 scope global secondary eth1:2  
    inet 172.28.128.200/24 scope global secondary eth1  
    inet 172.28.128.10/24 scope global secondary eth1:2  

**netology3 # for i in {1..50}; do curl -I -s 172.28.128.200>/dev/null; done**  

**netology2 # wc -l /var/log/nginx/access.log**  
75 /var/log/nginx/access.log  

**3)Включаем netology1**  
netology1# systemctl start nginx  
netology1#  ip -4 a | grep 172  
    inet 172.28.128.200/32 scope global lo  
    inet 172.28.128.11/24 brd 172.28.128.255 scope global eth1  
    inet 172.28.128.200/24 scope global secondary eth1  
    inet 172.28.128.10/24 scope global secondary eth1:1  

**3)Отключаем netology2**  
netology2# systemctl stop nginx  

**netology2 # ip -4 a | grep 172**    
    inet 172.28.128.200/32 scope global lo    
    inet 172.28.128.61/24 brd 172.28.128.255 scope global eth1  

**netology1# ip -4 a | grep 172**  
    inet 172.28.128.200/32 scope global lo  
    inet 172.28.128.11/24 brd 172.28.128.255 scope global eth1  
    inet 172.28.128.200/24 scope global secondary eth1  
    inet 172.28.128.10/24 scope global secondary eth1:1  
    inet 172.28.128.60/24 scope global secondary eth1:2  

**netology3 # for i in {1..50}; do curl -I -s 172.28.128.200>/dev/null; done  
netology1 # wc -l /var/log/nginx/access.log**  
75 /var/log/nginx/access.log   

**3)Вернули все назад**   
**netology2# systemctl start nginx**  

**netology1# ip -4 a | grep 172**  
    inet 172.28.128.200/32 scope global lo  
    inet 172.28.128.11/24 brd 172.28.128.255 scope global eth1  
    inet 172.28.128.200/24 scope global secondary eth1  
    inet 172.28.128.10/24 scope global secondary eth1:1  

**netology2# ip -4 a | grep 172**  
    inet 172.28.128.200/32 scope global lo  
    inet 172.28.128.61/24 brd 172.28.128.255 scope global eth1  
    inet 172.28.128.60/24 scope global secondary eth1:2  

**netology3 # for i in {1..50}; do curl -I -s 172.28.128.200>/dev/null; done**  

**netology1 # wc -l /var/log/nginx/access.log**   
100 /var/log/nginx/access.log  

**netology2 # wc -l /var/log/nginx/access.log**  
100 /var/log/nginx/access.log  





>>12. **Дополнительное задание вне зачета.** В прошлом примере и в лекции мы использовали только 1 адрес для балансировки. У такого подхода много отрицательных моментов, один из которых – невозможность активного использования нескольких хостов. Подумайте, сколько адресов оптимально использовать, если мы хотим без какой-либо деградации выдерживать потерю 1 из 3 хостов при входящем трафике 1.5 Гбит/с и физических линках хостов в 1 Гбит/с? Поясните логику своего ответа.  


С однозначным практическим решением не знаком, попробую ответить рассуждая логически.    
Так как у физического лика скорость 1 Гбит/с, то естественно 1 сервер не сможет обработать весь входящий поток.    
Распределить нагрузку между балансерами можно, например DNS – Round-Robin, но точность такого решения по делению трафика не высокая, поэтому если иметь 3 адреса (по одному на хост) с нагрузкой по 0.5 Гбит/с, то при выходе из строя 1 адреса получится на одном сервере нагрузка останется 0.5 Гбит/с , на другом уже недопустимая будет 1 Гбит/с (на грани, это без учета исходящего трафика).     

2 адреса вроде решат эту проблему, нагрузка по 0.75 Гбит/с, ставим на 3 хосте такой вес\приоритет, чтобы он забрал IP-адрес вышедшего из строя.  
Но этот вариант нельзя считать оптимальным, так как в штатной ситуации 1 сервер «простаивает». И что мы рассматривали выше, если Real-сервера находятся на тех же машинах и используется тож же сетевой интерфейс, то к 0.75 нужно прибавить нагрузку на пересылку трафика+ 2/3*0.75 =0.5   (тот сервер, что получит 2 IP 1/3*0.75 =0.25 ), тогда понятно, что опять сервер не справится с нагрузкой.
    
Тогда такое решение приходит на ум, во-первых, для связи с Real-серверами используем отдельный интерфейс.  
А каждому балансировщику выдать по 2 IP-адреса, тогда в случае отказа, 2 оставшихся сервера заберут по 1 IP и получать поровну нагрузку 0.75 Гбит/с, а в штатном режиме нагрузка будет также распределена поровну по 0.5 Гбит/с   

P.S. Для того чтобы вернуться в первоначальное состояние после сбоя кроме виртуальных IP-адресов потребуется ещё один «физический» на каждом хосте (чтобы хост мог сообщить, что он жив). В документации не увидел можно ли проверну жизни делать по другому интерфейсу, есть параметр «lvs_sync_daemon_inteface - specify the network interface for the LVS sync_daemon to run on», но как понял это в себя данную функцию не включает, и мультикаст состояния всегда идет через тот же интерфейс, где и виртуальные IP.      

